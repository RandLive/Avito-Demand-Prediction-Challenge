{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loading data ...\n",
      "loading data done!\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "incompatible type [object] for a datetime/timedelta operation",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-1e05b870c7c1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;31m#TODO####################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m \u001b[0mall_periods\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"days_wait\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mall_periods\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"date_from\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mall_periods\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"activation_date\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdays\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     62\u001b[0m \u001b[0mgp2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_periods\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgroupby\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"item_id\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"days_wait\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/ops.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(left, right, name, na_op)\u001b[0m\n\u001b[1;32m    718\u001b[0m         \u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_align_method_SERIES\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    719\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 720\u001b[0;31m         \u001b[0mconverted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_Op\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_op\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    721\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    722\u001b[0m         \u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconverted\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/ops.py\u001b[0m in \u001b[0;36mget_op\u001b[0;34m(cls, left, right, name, na_op)\u001b[0m\n\u001b[1;32m    347\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0m_Op\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_op\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    348\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 349\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0m_TimeOp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mright\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mna_op\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    350\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, left, right, name, na_op)\u001b[0m\n\u001b[1;32m    361\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    362\u001b[0m         \u001b[0mlvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_to_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mleft\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 363\u001b[0;31m         \u001b[0mrvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_convert_to_array\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mright\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    365\u001b[0m         \u001b[0;31m# left\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.6/site-packages/pandas/core/ops.py\u001b[0m in \u001b[0;36m_convert_to_array\u001b[0;34m(self, values, name, other)\u001b[0m\n\u001b[1;32m    521\u001b[0m             raise TypeError(\"incompatible type [{dtype}] for a \"\n\u001b[1;32m    522\u001b[0m                             \u001b[0;34m\"datetime/timedelta operation\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 523\u001b[0;31m                             .format(dtype=np.array(values).dtype))\n\u001b[0m\u001b[1;32m    524\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: incompatible type [object] for a datetime/timedelta operation"
     ]
    }
   ],
   "source": [
    "# %load LGB_Mengfei_multi_ridge_LB_0.2217.py\n",
    "from nltk.corpus import stopwords \n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.pipeline import FeatureUnion\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cross_validation import KFold\n",
    "from sklearn.linear_model import Ridge\n",
    "from scipy.sparse import hstack, csr_matrix\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import lightgbm as lgb\n",
    "import matplotlib.pyplot as plt\n",
    "import gc, re\n",
    "from sklearn.utils import shuffle\n",
    "from contextlib import contextmanager\n",
    "from sklearn.externals import joblib\n",
    "\n",
    "debug = True\n",
    "print(\"loading data ...\")\n",
    "used_cols = [\"item_id\", \"user_id\"]\n",
    "if debug == False:\n",
    "    train_df = pd.read_csv('input/train.csv', parse_dates = ['activation_date'])\n",
    "    y = train_df[\"deal_probability\"]\n",
    "    test_df = pd.read_csv('input/test.csv', parse_dates = ['activation_date'])\n",
    "    # suppl\n",
    "    train_active = pd.read_csv('input/train_active.csv', usecols=used_cols)\n",
    "    test_active = pd.read_csv('input/test_active.csv', usecols=used_cols)\n",
    "    train_periods = pd.read_csv('input/periods_train.csv', parse_dates=[\"date_from\", \"date_to\"])\n",
    "    test_periods = pd.read_csv('input/periods_test.csv', parse_dates=[\"date_from\", \"date_to\"])\n",
    "    #TODO new class#######################################\n",
    "#    new_class = pd.read_csv(\"new_image_class_f600000.csv\")\n",
    "else:\n",
    "    train_df = pd.read_csv('input/train.csv', parse_dates = ['activation_date'])\n",
    "    train_df = shuffle(train_df, random_state=1234); train_df = train_df.iloc[:5000]\n",
    "    y = train_df[\"deal_probability\"]\n",
    "    test_df = pd.read_csv('input/test.csv', nrows=1000, parse_dates = ['activation_date'])\n",
    "    # suppl \n",
    "    train_active = pd.read_csv('input/train_active.csv', nrows=1000, usecols=used_cols)\n",
    "    test_active = pd.read_csv('input/test_active.csv', nrows=1000, usecols=used_cols)\n",
    "    train_periods = pd.read_csv('input/periods_train.csv', nrows=1000, parse_dates=[\"date_from\", \"date_to\"])\n",
    "    test_periods = pd.read_csv('input/periods_test.csv', nrows=1000, parse_dates=[\"date_from\", \"date_to\"])\n",
    "    #TODO new class#######################################\n",
    "#    new_class = pd.read_csv(\"new_image_class_f600000.csv\")\n",
    "print(\"loading data done!\")\n",
    "\n",
    "# =============================================================================\n",
    "# Here Based on https://www.kaggle.com/bminixhofer/aggregated-features-lightgbm/code\n",
    "# =============================================================================\n",
    "all_samples = pd.concat([train_df,train_active,test_df,test_active]).reset_index(drop=True)\n",
    "all_samples.drop_duplicates([\"item_id\"], inplace=True)\n",
    "del train_active, test_active; gc.collect()\n",
    "\n",
    "all_periods = pd.concat([train_periods,test_periods])\n",
    "del train_periods, test_periods; gc.collect()\n",
    "\n",
    "all_periods[\"days_up\"] = (all_periods[\"date_to\"] - all_periods[\"date_from\"]).dt.days\n",
    "gp = all_periods.groupby([\"item_id\"])[[\"days_up\"]]\n",
    "\n",
    "#TODO####################################################\n",
    "all_periods[\"days_wait\"] = (all_periods[\"date_from\"] - all_periods[\"activation_date\"]).dt.days\n",
    "gp2 = all_periods.groupby([\"item_id\"])[[\"days_wait\"]]\n",
    "\n",
    "gp_df = pd.DataFrame()\n",
    "gp_df[\"days_up_sum\"] = gp.sum()[\"days_up\"]\n",
    "gp_df[\"times_put_up\"] = gp.count()[\"days_up\"]\n",
    "#TODO####################################################\n",
    "gp_df[\"days_wait_sum\"] = gp2.sum()[\"days_wait\"]\n",
    "gp_df[\"times_put_wait\"] = gp2.count()[\"days_wait\"]\n",
    "\n",
    "gp_df.reset_index(inplace=True)\n",
    "gp_df.rename(index=str, columns={\"index\": \"item_id\"})\n",
    "\n",
    "all_periods.drop_duplicates([\"item_id\"], inplace=True)\n",
    "all_periods = all_periods.merge(gp_df, on=\"item_id\", how=\"left\")\n",
    "all_periods = all_periods.merge(all_samples, on=\"item_id\", how=\"left\")\n",
    "\n",
    "\n",
    "gp = all_periods.groupby([\"user_id\"])[[\"days_up_sum\", \"times_put_up\"]].mean().reset_index()\\\n",
    ".rename(index=str, columns={\"days_up_sum\": \"avg_days_up_user\",\n",
    "                            \"times_put_up\": \"avg_times_up_user\"})\n",
    "#TODO####################################################\n",
    "gp2 = all_periods.groupby([\"user_id\"])[[\"days_wait_sum\", \"times_put_wait\"]].mean().reset_index()\\\n",
    ".rename(index=str, columns={\"days_wait_sum\": \"avg_days_wait_user\",\n",
    "                            \"times_put_wait\": \"avg_times_wait_user\"})\n",
    "\n",
    "n_user_items = all_samples.groupby([\"user_id\"])[[\"item_id\"]].count().reset_index() \\\n",
    ".rename(index=str, columns={\"item_id\": \"n_user_items\"})\n",
    "gp = gp.merge(n_user_items, on=\"user_id\", how=\"outer\") #left\n",
    "gp = gp.merge(gp2, on=\"user_id\", how=\"outer\")\n",
    "\n",
    "del all_samples, all_periods, n_user_items\n",
    "gc.collect()\n",
    "\n",
    "train_df = train_df.merge(gp, on=\"user_id\", how=\"left\")\n",
    "test_df = test_df.merge(gp, on=\"user_id\", how=\"left\")\n",
    "\n",
    "agg_cols = list(gp.columns)[1:]\n",
    "\n",
    "del gp; gc.collect()\n",
    "\n",
    "for col in agg_cols:\n",
    "    train_df[col].fillna(-1, inplace=True)\n",
    "    test_df[col].fillna(-1, inplace=True)\n",
    "\n",
    "print(\"merging supplimentary data done!\")\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# done! go to the normal steps\n",
    "# =============================================================================\n",
    "def rmse(predictions, targets):\n",
    "    print(\"calculating RMSE ...\")\n",
    "    return np.sqrt(((predictions - targets) ** 2).mean())\n",
    "\n",
    "def text_preprocessing(text):        \n",
    "    text = str(text)\n",
    "    text = text.lower() \n",
    "    # hash words\n",
    "    text = re.sub(r\"(\\\\u[0-9A-Fa-f]+)\",r\"\", text)\n",
    "    \n",
    "    text = re.sub(r\"===\",r\" \", text)\n",
    "    \n",
    "    # https://www.kaggle.com/demery/lightgbm-with-ridge-feature/code\n",
    "    text = \" \".join(map(str.strip, re.split('(\\d+)',text)))\n",
    "    regex = re.compile(u'[^[:alpha:]]')\n",
    "    text = regex.sub(\" \", text)\n",
    "    text = \" \".join(text.split())\n",
    "    return text\n",
    "\n",
    "@contextmanager\n",
    "def feature_engineering(df):\n",
    "    # All the feature engineering here  \n",
    "    \n",
    "    def Do_Text_Hash(df):\n",
    "        print(\"feature engineering -> hash text ...\")\n",
    "        df[\"text_feature\"] = df.apply(lambda row: \" \".join([str(row[\"param_1\"]),\n",
    "          str(row[\"param_2\"]), str(row[\"param_3\"])]),axis=1)\n",
    "    \n",
    "        df[\"text_feature_2\"] = df.apply(lambda row: \" \".join([str(row[\"param_2\"]), str(row[\"param_3\"])]),axis=1)        \n",
    "        df[\"title_description\"] = df.apply(lambda row: \" \".join([str(row[\"title\"]), str(row[\"description\"])]),axis=1)\n",
    "       \n",
    "        print(\"feature engineering -> preprocess text ...\")       \n",
    "        df[\"text_feature\"] = df[\"text_feature\"].apply(lambda x: text_preprocessing(x))\n",
    "        df[\"text_feature_2\"] = df[\"text_feature_2\"].apply(lambda x: text_preprocessing(x))\n",
    "        df[\"description\"] = df[\"description\"].apply(lambda x: text_preprocessing(x))\n",
    "        df[\"title\"] = df[\"title\"].apply(lambda x: text_preprocessing(x))\n",
    "        df[\"title_description\"] = df[\"title_description\"].apply(lambda x: text_preprocessing(x))\n",
    "                       \n",
    "    def Do_Datetime(df):\n",
    "        print(\"feature engineering -> date time ...\")\n",
    "        df[\"wday\"] = df[\"activation_date\"].dt.weekday\n",
    "#        df[\"week\"] = df[\"activation_date\"].dt.week\n",
    "#        df[\"dom\"] = df[\"activation_date\"].dt.day\n",
    "        \n",
    "    def Do_Label_Enc(df):\n",
    "        print(\"feature engineering -> lable encoding ...\")\n",
    "        lbl = LabelEncoder()\n",
    "        #TODO new class#######################################\n",
    "        cat_col = [\"user_id\", \"region\", \"city\", \"parent_category_name\",\n",
    "               \"category_name\", \"user_type\", 'image_top_1',\n",
    "               \"param_1\", \"param_2\", \"param_3\", 'image']\n",
    "        for col in cat_col:\n",
    "            df[col] = lbl.fit_transform(df[col].astype(str))\n",
    "            gc.collect()\n",
    "    \n",
    "    import string\n",
    "    count = lambda l1,l2: sum([1 for x in l1 if x in l2])         \n",
    "    def Do_NA(df):\n",
    "        print(\"feature engineering -> fill na ...\")\n",
    "#        df[\"price\"] = np.log(df[\"price\"]+0.001).astype(\"float32\")\n",
    "#        df[\"price\"].fillna(-1,inplace=True)\n",
    "#        df[\"image_top_1\"].fillna(-1,inplace=True)\n",
    "#        df[\"image_top_4\"].fillna(-1,inplace=True)\n",
    "        df[\"image\"].fillna(\"noinformation\",inplace=True)\n",
    "        df[\"param_1\"].fillna(\"nicapotato\",inplace=True)\n",
    "        df[\"param_2\"].fillna(\"nicapotato\",inplace=True)\n",
    "        df[\"param_3\"].fillna(\"nicapotato\",inplace=True)\n",
    "        df[\"title\"].fillna(\"nicapotato\",inplace=True)\n",
    "        df[\"description\"].fillna(\"nicapotato\",inplace=True)\n",
    "        \n",
    "    def Do_Count(df):  \n",
    "        print(\"feature engineering -> do count ...\")\n",
    "        # some count       \n",
    "        df[\"num_desc_punct\"] = df[\"description\"].apply(lambda x: count(x, set(string.punctuation)))\n",
    "        df[\"num_desc_capE\"] = df[\"description\"].apply(lambda x: count(x, \"[A-Z]\"))\n",
    "        df[\"num_desc_capP\"] = df[\"description\"].apply(lambda x: count(x, \"[А-Я]\"))\n",
    "        \n",
    "        df[\"num_title_punct\"] = df[\"title\"].apply(lambda x: count(x, set(string.punctuation)))\n",
    "        df[\"num_title_capE\"] = df[\"title\"].apply(lambda x: count(x, \"[A-Z]\"))\n",
    "        df[\"num_title_capP\"] = df[\"title\"].apply(lambda x: count(x, \"[А-Я]\"))               \n",
    "        # good, used, bad ... count\n",
    "        df[\"is_in_desc_хорошо\"] = df[\"description\"].str.contains(\"хорошо\").map({True:1, False:0})\n",
    "        df[\"is_in_desc_Плохо\"] = df[\"description\"].str.contains(\"Плохо\").map({True:1, False:0})\n",
    "        df[\"is_in_desc_новый\"] = df[\"description\"].str.contains(\"новый\").map({True:1, False:0})\n",
    "        df[\"is_in_desc_старый\"] = df[\"description\"].str.contains(\"старый\").map({True:1, False:0})\n",
    "        df[\"is_in_desc_используемый\"] = df[\"description\"].str.contains(\"используемый\").map({True:1, False:0})\n",
    "        df[\"is_in_desc_есплатная_доставка\"] = df[\"description\"].str.contains(\"есплатная доставка\").map({True:1, False:0})\n",
    "        df[\"is_in_desc_есплатный_возврат\"] = df[\"description\"].str.contains(\"есплатный возврат\").map({True:1, False:0})\n",
    "        df[\"is_in_desc_идеально\"] = df[\"description\"].str.contains(\"идеально\").map({True:1, False:0})\n",
    "        df[\"is_in_desc_подержанный\"] = df[\"description\"].str.contains(\"подержанный\").map({True:1, False:0})\n",
    "        df[\"is_in_desc_пСниженные_цены\"] = df[\"description\"].str.contains(\"Сниженные цены\").map({True:1, False:0})\n",
    "        \n",
    "                              \n",
    "    def Do_Drop(df):\n",
    "        df.drop([\"activation_date\", \"item_id\"], axis=1, inplace=True)\n",
    "        \n",
    "    def Do_Stat_Text(df):\n",
    "        print(\"feature engineering -> statistics in text ...\")\n",
    "        textfeats = [\"text_feature\",\"text_feature_2\",\"description\", \"title\"]\n",
    "        for col in textfeats:\n",
    "            df[col + \"_num_chars\"] = df[col].apply(len) \n",
    "            df[col + \"_num_words\"] = df[col].apply(lambda comment: len(comment.split()))\n",
    "            df[col + \"_num_unique_words\"] = df[col].apply(lambda comment: len(set(w for w in comment.split())))\n",
    "            df[col + \"_words_vs_unique\"] = df[col+\"_num_unique_words\"] / df[col+\"_num_words\"] * 100\n",
    "            gc.collect()\n",
    "                      \n",
    "    # choose which functions to run\n",
    "    Do_NA(df)\n",
    "    Do_Text_Hash(df)\n",
    "    Do_Label_Enc(df)\n",
    "    Do_Count(df)\n",
    "    Do_Datetime(df)   \n",
    "    Do_Stat_Text(df)       \n",
    "    Do_Drop(df)    \n",
    "    gc.collect()\n",
    "    return df\n",
    "\n",
    "def data_vectorize(df):\n",
    "    russian_stop = set(stopwords.words(\"russian\"))\n",
    "    tfidf_para = {\n",
    "    \"stop_words\": russian_stop,\n",
    "    \"analyzer\": \"word\",\n",
    "    \"token_pattern\": r\"\\w{1,}\",\n",
    "    \"sublinear_tf\": True,\n",
    "    \"dtype\": np.float32,\n",
    "    \"norm\": \"l2\",\n",
    "    #\"min_df\":5,\n",
    "    #\"max_df\":.9,\n",
    "    \"smooth_idf\":False\n",
    "    }\n",
    "    def get_col(col_name): return lambda x: x[col_name]\n",
    "    vectorizer = FeatureUnion([\n",
    "        (\"description\", TfidfVectorizer(\n",
    "            ngram_range=(1, 2),\n",
    "            max_features=18000,\n",
    "            **tfidf_para,\n",
    "            preprocessor=get_col(\"description\"))\n",
    "         ),\n",
    "        (\"title_description\", TfidfVectorizer(\n",
    "            ngram_range=(1, 2),\n",
    "            max_features=18000,\n",
    "            **tfidf_para,\n",
    "            preprocessor=get_col(\"title_description\"))\n",
    "         ),\n",
    "        (\"text_feature\", CountVectorizer(\n",
    "            ngram_range=(1, 2),\n",
    "            preprocessor=get_col(\"text_feature\"))\n",
    "         ),\n",
    "        (\"title\", TfidfVectorizer(\n",
    "            ngram_range=(1, 2),\n",
    "            **tfidf_para,\n",
    "            preprocessor=get_col(\"title\"))\n",
    "         ),\n",
    "    ])\n",
    "    vectorizer.fit(df.to_dict(\"records\"))\n",
    "    ready_full_df = vectorizer.transform(df.to_dict(\"records\"))    \n",
    "    tfvocab = vectorizer.get_feature_names()    \n",
    "    df.drop([\"text_feature\", \"text_feature_2\", \"description\",\"title\", \"title_description\"], axis=1, inplace=True)\n",
    "    df.fillna(-1, inplace=True)     \n",
    "    return df, ready_full_df, tfvocab\n",
    "# =============================================================================\n",
    "# Ridge feature https://www.kaggle.com/demery/lightgbm-with-ridge-feature/code\n",
    "# =============================================================================\n",
    "class SklearnWrapper(object):\n",
    "    def __init__(self, clf, seed=0, params=None, seed_bool = True):\n",
    "        if(seed_bool == True):\n",
    "            params['random_state'] = seed\n",
    "        self.clf = clf(**params)\n",
    "\n",
    "    def train(self, x_train, y_train):\n",
    "        self.clf.fit(x_train, y_train)\n",
    "\n",
    "    def predict(self, x):\n",
    "        return self.clf.predict(x)\n",
    "\n",
    "NFOLDS = 10#5\n",
    "SEED = 42\n",
    "def get_oof(clf, x_train, y, x_test):\n",
    "            \n",
    "    oof_train = np.zeros((len_train,))\n",
    "    oof_test = np.zeros((len_test,))\n",
    "    oof_test_skf = np.empty((NFOLDS, len_test))\n",
    "\n",
    "    for i, (train_index, test_index) in enumerate(kf):\n",
    "        print('Ridege oof Fold {}'.format(i))\n",
    "        x_tr = x_train[train_index]       \n",
    "        y = np.array(y)\n",
    "        y_tr = y[train_index]\n",
    "        x_te = x_train[test_index]      \n",
    "        clf.train(x_tr, y_tr)       \n",
    "        oof_train[test_index] = clf.predict(x_te)        \n",
    "        oof_test_skf[i, :] = clf.predict(x_test)\n",
    "    oof_test[:] = oof_test_skf.mean(axis=0)\n",
    "    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)\n",
    "\n",
    "\n",
    "full_df = pd.concat([train_df, test_df])\n",
    "#TODO new class#######################################\n",
    "#full_df = pd.merge(full_df, new_class, how='left', on=['image_top_1'])\n",
    "#full_df[\"new_class\"].fillna(\"beijing\",inplace=True)\n",
    "#print(full_df.head())\n",
    "sub_item_id = test_df[\"item_id\"]\n",
    "len_train = len(train_df)\n",
    "len_test = len(test_df)\n",
    "\n",
    "kf = KFold(len_train, n_folds=NFOLDS, shuffle=True, random_state=SEED)\n",
    "# =============================================================================\n",
    "# handle price\n",
    "# =============================================================================\n",
    "def feature_Eng_On_Price_SEQ(df):\n",
    "    print('feature engineering -> on price and SEQ ...')        \n",
    "    df[\"price\"] = np.log(df[\"price\"]+0.001).astype(\"float32\")\n",
    "    df[\"price\"].fillna(-1,inplace=True)     \n",
    "    df[\"price+\"] = np.round(df[\"price\"]*4.8).astype(int)   \n",
    "    df[\"item_seq_number+\"] = np.round(df[\"item_seq_number\"]/100).astype(int) \n",
    "    return df\n",
    "\n",
    "train_df, val_df = train_test_split(\n",
    "    full_df.iloc[:len_train], test_size=0.1, random_state=42) #23    \n",
    "def feature_Eng_On_Deal_Prob(df, df_train):\n",
    "    print('feature engineering -> on price deal prob +...')\n",
    "    df2 = df    \n",
    "    tmp = df_train.groupby([\"price+\"], as_index=False)['deal_probability'].median().rename(columns={'deal_probability':'median_deal_probability_price+'})     \n",
    "    df = pd.merge(df, tmp, how='left', on=[\"price+\"])\n",
    "    df2['median_deal_probability_price+'] = df['median_deal_probability_price+']  \n",
    "    del tmp; gc.collect()\n",
    "    \n",
    "    tmp = df_train.groupby([\"item_seq_number+\"], as_index=False)['deal_probability'].median().rename(columns={'deal_probability':'median_deal_probability_item_seq_number+'})     \n",
    "    df = pd.merge(df, tmp, how='left', on=[\"item_seq_number+\"])\n",
    "    df2['median_deal_probability_item_seq_number+'] = df['median_deal_probability_item_seq_number+']\n",
    "    \n",
    "    #TODO################################################\n",
    "#    tmp = df_train.groupby([\"parent_category_name\"], as_index=False)['deal_probability'].median().rename(columns={'deal_probability':'median_deal_probability_parent_category_name'})     \n",
    "#    df = pd.merge(df, tmp, how='left', on=[\"parent_category_name\"])\n",
    "#    df2['median_deal_probability_parent_category_name'] = df['median_deal_probability_parent_category_name']\n",
    "    \n",
    "#    tmp = df_train.groupby([\"category_name\"], as_index=False)['deal_probability'].median().rename(columns={'deal_probability':'median_deal_probability_category_name'})     \n",
    "#    df = pd.merge(df, tmp, how='left', on=[\"category_name\"])\n",
    "#    df2['median_deal_probability_category_name'] = df['median_deal_probability_category_name']\n",
    "    \n",
    "#    tmp = df_train.groupby([\"user_type\"], as_index=False)['deal_probability'].median().rename(columns={'deal_probability':'median_deal_probability_user_type'})     \n",
    "#    df = pd.merge(df, tmp, how='left', on=[\"user_type\"])\n",
    "#    df2['median_deal_probability_user_type'] = df['median_deal_probability_user_type']\n",
    "    \n",
    "    tmp = df_train.groupby([\"image_top_1\"], as_index=False)['deal_probability'].median().rename(columns={'deal_probability':'median_deal_probability_image_top_1'})     \n",
    "    df = pd.merge(df, tmp, how='left', on=[\"image_top_1\"])\n",
    "    df2['median_deal_probability_image_top_1'] = df['median_deal_probability_image_top_1']\n",
    "    \n",
    "#    tmp = df_train.groupby([\"param_1\"], as_index=False)['deal_probability'].median().rename(columns={'deal_probability':'median_deal_probability_param_1'})     \n",
    "#    df = pd.merge(df, tmp, how='left', on=[\"param_1\"])\n",
    "#    df2['median_deal_probability_param_1'] = df['median_deal_probability_param_1']\n",
    "    \n",
    "#    tmp = df_train.groupby([\"param_2\"], as_index=False)['deal_probability'].median().rename(columns={'deal_probability':'median_deal_probability_param_2'})     \n",
    "#    df = pd.merge(df, tmp, how='left', on=[\"param_2\"])\n",
    "#    df2['median_deal_probability_param_2'] = df['median_deal_probability_param_2']\n",
    "       \n",
    "    df2.fillna(-1, inplace=True)    \n",
    "    del tmp; gc.collect()\n",
    "    return df2\n",
    "\n",
    "del full_df['deal_probability']; gc.collect()\n",
    "\n",
    "# =============================================================================\n",
    "# use additianl image data\n",
    "# =============================================================================\n",
    "feature_engineering(full_df)\n",
    "\n",
    "feature_Eng_On_Price_SEQ(full_df)\n",
    "feature_Eng_On_Price_SEQ(train_df)\n",
    "feature_Eng_On_Deal_Prob(full_df, train_df)\n",
    "\n",
    "del train_df, test_df; gc.collect()\n",
    "full_df, ready_full_df, tfvocab = data_vectorize(full_df)\n",
    "\n",
    "#'alpha':20.0\n",
    "ridge_params = {'alpha':20.0, 'fit_intercept':True, 'normalize':False, 'copy_X':True,\n",
    "                'max_iter':None, 'tol':0.001, 'solver':'auto', 'random_state':SEED}\n",
    "ridge = SklearnWrapper(clf=Ridge, seed = SEED, params = ridge_params)\n",
    "ready_df = ready_full_df\n",
    "\n",
    "print('ridge 1 oof ...')\n",
    "ridge_oof_train, ridge_oof_test = get_oof(ridge, np.array(full_df)[:len_train], y, np.array(full_df)[len_train:])\n",
    "ridge_preds = np.concatenate([ridge_oof_train, ridge_oof_test])\n",
    "full_df['ridge_preds_1'] = ridge_preds\n",
    "full_df['ridge_preds_1'].clip(0.0, 1.0, inplace=True)\n",
    "\n",
    "print('ridge 2 oof ...')\n",
    "ridge_oof_train, ridge_oof_test = get_oof(ridge, ready_df[:len_train], y, ready_df[len_train:])\n",
    "ridge_preds = np.concatenate([ridge_oof_train, ridge_oof_test])\n",
    "full_df['ridge_preds_2'] = ridge_preds\n",
    "full_df['ridge_preds_2'].clip(0.0, 1.0, inplace=True)\n",
    "\n",
    "\n",
    "print(\"Modeling Stage ...\")\n",
    "# Combine Dense Features with Sparse Text Bag of Words Features\n",
    "X = hstack([csr_matrix(full_df.iloc[:len_train]), ready_full_df[:len_train]]) # Sparse Matrix\n",
    "test = hstack([csr_matrix(full_df.iloc[len_train:]), ready_full_df[len_train:]]) # Sparse Matrix\n",
    "tfvocab = full_df.columns.tolist() + tfvocab\n",
    "\n",
    "\n",
    "for shape in [X,test]:\n",
    "    print(\"{} Rows and {} Cols\".format(*shape.shape))\n",
    "print(\"Feature Names Length: \",len(tfvocab))\n",
    "#TODO new class#######################################\n",
    "cat_col = [\n",
    "           \"user_id\",\n",
    "           \"region\", \n",
    "           \"city\", \n",
    "           \"parent_category_name\",\n",
    "           \"category_name\", \n",
    "           \"user_type\", \n",
    "           \"image_top_1\",\n",
    "           \"param_1\", \n",
    "           \"param_2\", \n",
    "           \"param_3\",\n",
    "           \"price+\",\n",
    "           \"item_seq_number+\",\n",
    "#           \"new_class\"\n",
    "           ]\n",
    "\n",
    "rmse_sume = 0.\n",
    "\n",
    "for numIter in range(0, 1):\n",
    "      \n",
    "      X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size=0.1, random_state=42) #23\n",
    "      \n",
    "#      X_train, X_valid = X.tocsr()[train_index], X.tocsr()[test_index]\n",
    "#      y_train, y_valid = y.iloc[train_index], y.iloc[test_index]\n",
    "     \n",
    "      lgbm_params =  {\n",
    "              \"tree_method\": \"feature\",    \n",
    "              \"num_threads\": 3,\n",
    "              \"task\": \"train\",\n",
    "              \"boosting_type\": \"gbdt\",\n",
    "              \"objective\": \"regression\",\n",
    "              \"metric\": \"rmse\",\n",
    "      #        \"max_depth\": 15,\n",
    "              \"num_leaves\": 280, # 35\n",
    "              \"feature_fraction\": 0.6,\n",
    "              \"bagging_fraction\": 0.6,\n",
    "              \"learning_rate\": 0.019,\n",
    "              \"verbose\": -1,\n",
    "              'lambda_l1':1,\n",
    "              'lambda_l2':1,\n",
    "              }\n",
    "      \n",
    "      lgtrain = lgb.Dataset(X_train, y_train,\n",
    "                      feature_name=tfvocab,\n",
    "                      categorical_feature = cat_col)\n",
    "      lgvalid = lgb.Dataset(X_valid, y_valid,\n",
    "                      feature_name=tfvocab,\n",
    "                      categorical_feature = cat_col)\n",
    "      lgb_clf = lgb.train(\n",
    "              lgbm_params,\n",
    "              lgtrain,\n",
    "              num_boost_round=32000,\n",
    "              valid_sets=[lgtrain, lgvalid],\n",
    "              valid_names=[\"train\",\"valid\"],\n",
    "              early_stopping_rounds=200,\n",
    "              verbose_eval=100, #200\n",
    "              )\n",
    "      \n",
    "      print(\"save model ...\")\n",
    "      joblib.dump(lgb_clf, \"lgb_{}.pkl\".format(numIter))\n",
    "      ## load model\n",
    "      #lgb_clf = joblib.load(\"lgb.pkl\")\n",
    "      \n",
    "      print(\"Model Evaluation Stage\")\n",
    "      print( \"RMSE:\", rmse(y_valid, lgb_clf.predict(X_valid, num_iteration=lgb_clf.best_iteration)) )\n",
    "      lgpred = lgb_clf.predict(test, num_iteration=lgb_clf.best_iteration)\n",
    "      \n",
    "      lgsub = pd.DataFrame(lgpred,columns=[\"deal_probability\"],index=sub_item_id)\n",
    "      lgsub[\"deal_probability\"].clip(0.0, 1.0, inplace=True) # Between 0 and 1\n",
    "      lgsub.to_csv(\"ml_lgb_sub_{}.csv\".format(numIter),index=True,header=True)\n",
    "\n",
    "      rmse_sume += rmse(y_valid, lgb_clf.predict(X_valid, num_iteration=lgb_clf.best_iteration))\n",
    "      \n",
    "      numIter += 1\n",
    "      \n",
    "      del X_train, X_valid, y_train, y_valid, lgtrain, lgvalid\n",
    "      gc.collect()\n",
    "\n",
    "print(\"mean rmse is:\", rmse_sume/5)\n",
    "      \n",
    "print(\"Features importance...\")\n",
    "bst = lgb_clf\n",
    "gain = bst.feature_importance(\"gain\")\n",
    "ft = pd.DataFrame({\"feature\":bst.feature_name(), \"split\":bst.feature_importance(\"split\"), \"gain\":100 * gain / gain.sum()}).sort_values(\"gain\", ascending=False)\n",
    "print(ft.head(50))\n",
    "\n",
    "plt.figure()\n",
    "ft[[\"feature\",\"gain\"]].head(50).plot(kind=\"barh\", x=\"feature\", y=\"gain\", legend=False, figsize=(10, 20))\n",
    "plt.gcf().savefig(\"features_importance.png\")\n",
    "\n",
    "print(\"Done.\")\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "mean rmse is: 0.21697176669518864\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
